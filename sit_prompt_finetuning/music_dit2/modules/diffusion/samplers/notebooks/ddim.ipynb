{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.102210Z",
     "start_time": "2024-06-28T06:28:05.363913Z"
    }
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Tuple"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Beta schedule",
   "id": "a08e90f88f573dcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.107038Z",
     "start_time": "2024-06-28T06:28:06.103064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def betas_for_alpha_bar(num_diffusion_time_steps: int,\n",
    "                        max_beta: float = 0.999,\n",
    "                        alpha_transform_type: str = \"cosine\"):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function, which defines the cumulative product of\n",
    "    (1-beta) over time from t = [0,1].\n",
    "\n",
    "    Contains a function alpha_bar that takes an argument t and transforms it to the cumulative product of (1-beta) up\n",
    "    to that part of the diffusion process.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        num_diffusion_time_steps (`int`): the number of betas to produce.\n",
    "        max_beta (`float`): the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "        alpha_transform_type (`str`, *optional*, default to `cosine`): the type of noise schedule for alpha_bar.\n",
    "                     Choose from `cosine` or `exp`\n",
    "\n",
    "    Returns:\n",
    "        betas (`np.ndarray`): the betas used by the scheduler to step the model outputs\n",
    "    \"\"\"\n",
    "    if alpha_transform_type == \"cosine\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2\n",
    "\n",
    "    elif alpha_transform_type == \"exp\":\n",
    "\n",
    "        def alpha_bar_fn(t):\n",
    "            return math.exp(t * -12.0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported alpha_transform_type: {alpha_transform_type}\")\n",
    "\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_time_steps):\n",
    "        t1 = i / num_diffusion_time_steps\n",
    "        t2 = (i + 1) / num_diffusion_time_steps\n",
    "        betas.append(min(1 - alpha_bar_fn(t2) / alpha_bar_fn(t1), max_beta))\n",
    "    return torch.tensor(betas, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def make_beta_schedule(schedule: str,\n",
    "                       num_steps: int,\n",
    "                       beta_start: float = 1e-4,\n",
    "                       beta_end: float = 2e-2) -> torch.Tensor:\n",
    "    \"\"\"Make a scheduled sequence of betas.\n",
    "\n",
    "    Args:\n",
    "        schedule (str):         The schedule type.\n",
    "        num_steps (int):        The number of time steps.\n",
    "        beta_start (float):     The start value of the linear schedule.\n",
    "        beta_end (float):       The end value of the linear schedule.\n",
    "    \"\"\"\n",
    "    if schedule == \"linear\":\n",
    "        return torch.linspace(beta_start, beta_end, num_steps, dtype=torch.float32)\n",
    "    elif schedule == \"scaled_linear\":\n",
    "        # this schedule is very specific to the latent diffusion model.\n",
    "        return torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_steps, dtype=torch.float32) ** 2\n",
    "    elif schedule == \"squaredcos_cap_v2\":\n",
    "        # Glide cosine schedule\n",
    "        return betas_for_alpha_bar(num_steps)\n",
    "    elif schedule == \"sigmoid\":\n",
    "        # GeoDiff sigmoid schedule\n",
    "        betas = torch.linspace(-6, 6, num_steps, dtype=torch.float32)\n",
    "        return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' is unknown.\")"
   ],
   "id": "1074541ba02a4c9",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.116223Z",
     "start_time": "2024-06-28T06:28:06.107921Z"
    }
   },
   "cell_type": "code",
   "source": "BETA_SCHEDULE = \"linear\"",
   "id": "cbf7e27af3b31940",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.183466Z",
     "start_time": "2024-06-28T06:28:06.117007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "betas = make_beta_schedule(schedule=BETA_SCHEDULE,\n",
    "                           num_steps=1000,\n",
    "                           beta_start=1e-4,\n",
    "                           beta_end=0.02)\n",
    "plt.plot(betas, label=\"beta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b93e37decf22d806",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Alphas\n",
    "\n",
    "$$\\alpha_t = 1 - \\beta_t$$\n",
    "$$\\bar{\\alpha}_t = \\prod_{s=0}^{t}\\alpha_s$$\n",
    "$$\\vec{x}_t = \\sqrt{\\bar{\\alpha}_t} \\cdot \\vec{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\vec{\\epsilon}_0$$"
   ],
   "id": "9198d939928a47d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.259755Z",
     "start_time": "2024-06-28T06:28:06.184930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "sqrt_alphas_cumprod = alphas_cumprod ** 0.5\n",
    "sqrt_betas_cumprod = (1 - alphas_cumprod) ** 0.5\n",
    "\n",
    "plt.plot(sqrt_alphas_cumprod, label=\"sqrt_alphas_cumprod (coeff for x_0)\")\n",
    "plt.plot(sqrt_betas_cumprod, label=\"sqrt_betas_cumprod (coeff for noise)\")\n",
    "plt.legend()\n",
    "plt.plot()"
   ],
   "id": "7f15e2d2d7b88251",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Add noise\n",
    "\n",
    "$$\\vec{x}_t = \\sqrt{\\bar{\\alpha}_t} \\cdot \\vec{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\vec{\\epsilon}_0$$\n",
    "$$\\vec{x}_t \\sim q(\\vec{x}_t|\\vec{x}_0) = N(\\vec{x}_t; \\sqrt{\\bar{\\alpha}_t} \\vec{x}_0, (1 - \\bar{\\alpha}) \\vec{I}) $$"
   ],
   "id": "cd78b6affe0d9fdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.263315Z",
     "start_time": "2024-06-28T06:28:06.260501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_noise(x: torch.Tensor,\n",
    "              noise: torch.Tensor,\n",
    "              timesteps: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Add noise to the input tensor.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor):               The original samples.  Shape: (batch_size, ...).\n",
    "        noise (torch.Tensor):           The noise tensor to be added.  Shape should be the same as x.\n",
    "        timesteps (torch.Tensor):       Time steps for each batch.  Shape: (batch_size,).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor:                   The noisy samples.\n",
    "    \"\"\"\n",
    "    assert x.size() == noise.size(), \\\n",
    "        f\"The size of x ({x.size()}) and noise ({noise.size()}) should be the same.\"\n",
    "    \n",
    "    timesteps = timesteps.long()\n",
    "    \n",
    "    assert timesteps.dim() == 1, \\\n",
    "        f\"The timesteps should be a 1D tensor, but got {timesteps.dim()}D.\"\n",
    "    assert x.size(0) == timesteps.size(0), \\\n",
    "        f\"The batch size of x ({x.size(0)}) and timesteps ({timesteps.size(0)}) should be the same.\"\n",
    "    \n",
    "    assert 0 <= timesteps.min() <= timesteps.max() < 1000, \\\n",
    "        (f\"The timesteps should be in the range of [0, {999}], \"\n",
    "         f\"but got {timesteps.min()} to {timesteps.max()}.\")\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[timesteps].flatten()\n",
    "    sqrt_betas_cumprod_t = sqrt_betas_cumprod[timesteps].flatten()\n",
    "    \n",
    "    while sqrt_alphas_cumprod_t.dim() < x.dim():\n",
    "        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.unsqueeze(-1)\n",
    "        sqrt_betas_cumprod_t = sqrt_betas_cumprod_t.unsqueeze(-1)\n",
    "    \n",
    "    noisy_samples = sqrt_alphas_cumprod_t * x + sqrt_betas_cumprod_t * noise\n",
    "    \n",
    "    return noisy_samples"
   ],
   "id": "360ba2a1e364a7f6",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.272063Z",
     "start_time": "2024-06-28T06:28:06.264044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_0 = torch.randn(3, 10, 16)\n",
    "noise = torch.randn(3, 10, 16)\n",
    "timesteps = torch.randint(0, 1000, (3,))\n",
    "x_t = add_noise(x_0, noise, timesteps)\n",
    "assert x_0.shape == x_t.shape\n",
    "print(\"Time steps:\", timesteps)"
   ],
   "id": "4b9a9482f636f87e",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Posterior mean and variance\n",
    "\n",
    "$$q(\\vec{x}_{t - 1} | \\vec{x}_t, \\vec{x}_0) = N(\\vec{x}; \\tilde{\\mu}_t (\\vec{x}_t, \\vec{x}_0), \\tilde{\\beta}_t \\vec{I})$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\tilde{\\mu}_t (\\vec{x}_t, \\vec{x}_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\  \\beta_t}{1-\\bar{\\alpha}_t} \\vec{x}_0 \\  + \\ \\frac{\\sqrt{\\alpha_t} \\ (1 - \\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t} \\vec{x}_t$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\tilde{\\beta}_t = \\frac{1 - \\bar{\\alpha}_{t - 1}}{1 - \\bar{\\alpha}_t} \\beta_t$$"
   ],
   "id": "f77150a3629cdc38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.277791Z",
     "start_time": "2024-06-28T06:28:06.272751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "alphas_cumprod_prev = torch.cat([torch.Tensor([1.0]), alphas_cumprod[:-1]])\n",
    "assert alphas_cumprod_prev.shape == alphas_cumprod.shape\n",
    "print(alphas_cumprod[0:8])\n",
    "print(alphas_cumprod_prev[0:8])"
   ],
   "id": "35d5ef0187753457",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.283084Z",
     "start_time": "2024-06-28T06:28:06.278427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "posterior_mean_coeff1 = betas * (alphas_cumprod_prev ** 0.5) / (1.0 - alphas_cumprod)\n",
    "posterior_mean_coeff2 = (1.0 - alphas_cumprod_prev) * (alphas ** 0.5) / (1 - alphas_cumprod)"
   ],
   "id": "46a1b641cf988bdd",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.291408Z",
     "start_time": "2024-06-28T06:28:06.283796Z"
    }
   },
   "cell_type": "code",
   "source": "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)",
   "id": "1e93d36935cd6251",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.402646Z",
     "start_time": "2024-06-28T06:28:06.292084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_, (plt1, plt2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt1.plot(posterior_variance, label=\"posterior_variance\")\n",
    "plt1.legend()\n",
    "plt2.plot(posterior_mean_coeff1, label=\"posterior_mean_coeff1\")\n",
    "plt2.plot(posterior_mean_coeff2, label=\"posterior_mean_coeff2\")\n",
    "plt2.legend()\n",
    "plt.show()"
   ],
   "id": "44022f4d490838d6",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.455692Z",
     "start_time": "2024-06-28T06:28:06.403432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "posterior_log_variance_clipped = torch.log(torch.cat([posterior_variance[1:2], posterior_variance[1:]])) \n",
    "plt.plot(posterior_log_variance_clipped, label=\"posterior_log_variance_clipped\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f8dba6d504e95959",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.459629Z",
     "start_time": "2024-06-28T06:28:06.456854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_into_tensor(arr: torch.Tensor,\n",
    "                        timesteps: torch.Tensor,\n",
    "                        broadcast_shape: torch.Size) -> torch.Tensor:\n",
    "    \"\"\"Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    Args:\n",
    "        arr:                            the 1-D numpy array.\n",
    "        timesteps:                      a tensor of indices into the array to extract.\n",
    "        broadcast_shape:                a larger shape of K dimensions with the batch dimension equal to\n",
    "                                        the length of timesteps.\n",
    "\n",
    "    Returns:\n",
    "        a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = arr.to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res + torch.zeros(broadcast_shape, device=timesteps.device)\n",
    "\n",
    "def q_posterior_mean_variance(samples: torch.Tensor,\n",
    "                              noisy_samples: torch.Tensor,\n",
    "                              timesteps: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute the mean and variance of the diffusion posterior:\n",
    "        q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "    Refer to the equations (6) and (7) of the DDPM paper https://arxiv.org/abs/2006.11239 for more details.\n",
    "\n",
    "    Args:\n",
    "        samples (torch.Tensor):         The original samples.  Shape: (batch_size, ...).\n",
    "        noisy_samples (torch.Tensor):   The noisy samples returned by add_noise.  Shape: (batch_size, ...).\n",
    "        timesteps (torch.Tensor):       Time steps for each batch.  Shape: (batch_size,).\n",
    "\n",
    "    Returns:    Tuple of two tensors:\n",
    "        torch.Tensor:                   The mean of the posterior.\n",
    "        torch.Tensor:                   The clipped log variance of the posterior.\n",
    "    \"\"\"\n",
    "    posterior_mean = (extract_into_tensor(posterior_mean_coeff1,\n",
    "                                          timesteps,\n",
    "                                          samples.shape) * samples\n",
    "                    + extract_into_tensor(posterior_mean_coeff2,\n",
    "                                          timesteps,\n",
    "                                          samples.shape) * noisy_samples)\n",
    "\n",
    "    posterior_log_variance = extract_into_tensor(posterior_log_variance_clipped,\n",
    "                                                 timesteps,\n",
    "                                                 samples.shape)\n",
    "\n",
    "    return posterior_mean, posterior_log_variance"
   ],
   "id": "830842234c79e06d",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.468317Z",
     "start_time": "2024-06-28T06:28:06.460809Z"
    }
   },
   "cell_type": "code",
   "source": "x_tm1_mean, x_tm1_logvar = q_posterior_mean_variance(x_0, x_t, timesteps)",
   "id": "55471a15daf97917",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.474963Z",
     "start_time": "2024-06-28T06:28:06.469079Z"
    }
   },
   "cell_type": "code",
   "source": "print((x_tm1_mean - x_t).mean())",
   "id": "fcd7ab050b7991d2",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.481043Z",
     "start_time": "2024-06-28T06:28:06.475753Z"
    }
   },
   "cell_type": "code",
   "source": "alphas_cumprod_prev[0]",
   "id": "ce6997094da22c1d",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the DDIM paper equation (16), the variance is calculated as\n",
    "$$\\sigma^2_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\left(1 - \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}} \\right). $$\n",
    "Since the latter term $$1 - \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}} = \\beta_t,$$\n",
    "we have\n",
    "$$\\sigma^2_t = \\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\beta_i = \\tilde{\\beta}_t.$$"
   ],
   "id": "530c4d82e7a03d80"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.486257Z",
     "start_time": "2024-06-28T06:28:06.481676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "var2 = (1 - alphas_cumprod_prev) / (1 - alphas_cumprod) * (1 - alphas_cumprod / alphas_cumprod_prev)\n",
    "assert (var2 - posterior_variance).max() < 1e-6"
   ],
   "id": "a0a71b9811a5b054",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DDIM Denoise",
   "id": "7bc8a5425694b84e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.493449Z",
     "start_time": "2024-06-28T06:28:06.487107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference timesteps:\n",
    "def update_time_steps(num_training_steps: int,\n",
    "                      num_inference_steps: int,\n",
    "                      timestep_spacing: str = \"leading\",\n",
    "                      steps_offset: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Update the discrete time steps used for the diffusion chain (to be run before inference).\n",
    "\n",
    "    Args:\n",
    "        num_training_steps (int):       The number of training steps.\n",
    "        num_inference_steps (int):      The number of inference steps.\n",
    "        timestep_spacing (str):         The spacing of the time steps. Could be one of \"linspace\", \"leading\", or\n",
    "                                        \"trailing\". Defaults to \"leading\".\n",
    "        steps_offset (int):             The offset of the inference steps. Defaults to 0.\n",
    "    Returns:\n",
    "        time_steps (torch.Tensor):      The time steps.\n",
    "    \"\"\"\n",
    "    assert num_inference_steps <= num_training_steps, \\\n",
    "        (f\"The number of inference steps ({num_inference_steps}) should be less than \"\n",
    "         f\"the number of training steps ({num_training_steps}).\")\n",
    "\n",
    "    if timestep_spacing == \"linspace\":\n",
    "        time_steps = torch.linspace(0, num_training_steps - 1, num_inference_steps).flip(0).round().long()\n",
    "    elif timestep_spacing == \"leading\":\n",
    "        step_ratio = num_training_steps // num_inference_steps\n",
    "        # creates integer time steps by multiplying by ratio\n",
    "        # casting to int to avoid issues when num_inference_step is power of 3\n",
    "        time_steps = (torch.arange(0, num_inference_steps) * step_ratio).flip(0).round().long()\n",
    "        time_steps += steps_offset\n",
    "    elif timestep_spacing == \"trailing\":\n",
    "        step_ratio = num_training_steps / num_inference_steps\n",
    "        # creates integer time steps by multiplying by ratio\n",
    "        # casting to int to avoid issues when num_inference_step is power of 3\n",
    "        time_steps = (torch.arange(num_training_steps, 0, -step_ratio)).round().long()\n",
    "        time_steps -= 1\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"{timestep_spacing} is not supported. Please make sure to \"\n",
    "            f\"choose one of 'leading' or 'trailing' or 'linspace'.\"\n",
    "        )\n",
    "\n",
    "    assert time_steps.shape == (num_inference_steps,)\n",
    "\n",
    "    return time_steps\n",
    "\n",
    "inference_timesteps = update_time_steps(1000, 10, \"leading\")\n",
    "print(inference_timesteps)"
   ],
   "id": "1d13d673a1c0daf4",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.500172Z",
     "start_time": "2024-06-28T06:28:06.494138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_timestep = torch.Tensor([300, 400, 200]).long()\n",
    "prev_timestep = torch.Tensor([200, 300, 100]).long()\n",
    "sqrt_betas_cumprod_t = sqrt_betas_cumprod[current_timestep]\n",
    "sqrt_alphas_cumprod_t = sqrt_alphas_cumprod[current_timestep]\n",
    "alphas_cumprod_prev_t = alphas_cumprod[prev_timestep]"
   ],
   "id": "2abc68caab278871",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate variance",
   "id": "a45a87bee7073f48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.566679Z",
     "start_time": "2024-06-28T06:28:06.500898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_log_variance = posterior_log_variance_clipped\n",
    "max_log_variance = torch.log(betas)\n",
    "assert torch.all(min_log_variance <= max_log_variance)\n",
    "\n",
    "log_var = torch.randint(0, 1000, (1000,)) / 2000.0 - 1.0\n",
    "log_var = 0.5 * (log_var + 1) * (max_log_variance - min_log_variance) + min_log_variance\n",
    "variance = torch.exp(log_var)\n",
    "\n",
    "plt.plot(min_log_variance, label='min_log_variance')\n",
    "plt.plot(max_log_variance, label='max_log_variance')\n",
    "plt.plot(log_var, label='log_var')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "9ecca24742899869",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.568944Z",
     "start_time": "2024-06-28T06:28:06.567416Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "269ccb56cd6b3bce",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Predict $x_0$ by DDIM Paper Equation (12)\n",
    "\n",
    "$$\\hat{x}_0 = \\frac{\\vec{x}_t - \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\epsilon_\\theta^{(t)}(\\vec{x}_t) }{\\sqrt{\\bar{\\alpha}_t}} $$"
   ],
   "id": "3ffb03a4bb03549e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:28:06.572298Z",
     "start_time": "2024-06-28T06:28:06.569658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_output = noise\n",
    "sqrt_betas_cumprod_t = sqrt_betas_cumprod_t.view(3, *([1] * (x_t.dim() - 1)))\n",
    "sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t.view(3, *([1] * (x_t.dim() - 1)))\n",
    "pred_original_sample = (x_t - sqrt_betas_cumprod_t * model_output) / sqrt_alphas_cumprod_t\n",
    "assert pred_original_sample.shape == x_t.shape"
   ],
   "id": "1606169feebaecad",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict direction pointing to $x_t$",
   "id": "6d34a70c8d81a1fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:29:44.848490Z",
     "start_time": "2024-06-28T06:29:44.845817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eta = 0.1\n",
    "sigma = eta * (variance[current_timestep] ** 0.5)\n",
    "alphas_cumprod_prev_t = alphas_cumprod_prev_t.view(3,  *([1] * (x_t.dim() - 1)))\n",
    "sigma = sigma.view(3,  *([1] * (x_t.dim() - 1)))\n",
    "pred_sample_direction = ((1 - alphas_cumprod_prev_t - sigma**2) ** 0.5) * model_output"
   ],
   "id": "73f5d74a0910492d",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calculate the previous sample",
   "id": "7febea0fb51856af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T06:32:03.626832Z",
     "start_time": "2024-06-28T06:32:03.624704Z"
    }
   },
   "cell_type": "code",
   "source": "prev_sample = (alphas_cumprod_prev_t ** 0.5) * pred_original_sample + pred_sample_direction",
   "id": "d9c4c67f14e5d355",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8114e398c234c076",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
